{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2: Regression\n",
    "\n",
    "We will look at Single Linear Regression, Multi-Linear Regression and a simpler Non-Linear Regression (Polynomial Regression). There are many advanced forms of regression that we won't touch upon including those using kernels, gaussian processes, neural nets, decision trees, xgboost etc. The idea here though is to get a thorough feel for key concepts. However gaining a quick familiarity with SciKit will enable you to play with more powerful models.\n",
    "\n",
    "On the coding side we will use Python, Numpy, Pandas and SciKit Learn. Some of the simpler algorithms we will code up and then compare with Scikit's implementation.\n",
    "\n",
    "Regression is a bit too broad to go through in one lab to be honest, but we will touch upon over-fitting and regularisation and try to give some intuitions, as well handling training data. For the most part we will either use the Normal equations or Scikit's library. The normal equations become unwieldy with large amounts of data, so at the end we will touch upon gradient descent.\n",
    "\n",
    "If this is your first view of regression, Sci-Kit learn and Python then it is a big area, so don't worry too much. The main idea is to introduce some tools. The actual amount of code implementation is a small proportion of the notebook and for those less familiar with Python just understand the high level view of what is happening and be able to use the tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Some of the code in this tutorial originally comes from code accompanying 'Hand's on Machine Learning sith Scikit-Learn and Tensorflow', by Geron, also the complete tutorial to ridge regression and lasso in python.\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\n",
    "\n",
    "A basic tutorial on Sci-Kit learn may be found at:\n",
    "\n",
    "http://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "\n",
    "and \n",
    "\n",
    "https://www.youtube.com/watch?v=2kT6QOVSgSg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Please do not change the cell below, you will see a number of imports. If a Data Scientist uses Python then numpy, pandas and matplotlib are pretty much guaranteed to be part of his toolbox. Scikit learn is a library containing various machine learning models, datasets and useful utilities for handling the data. If you continue in this area it is advisable to become familiar with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "\n",
    "#Standard python libraries for data and visualisation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#SciKit Learn a python ML Library\n",
    "#Import models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "#Import error metric\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Import a dataset\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "#Import data munging tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#Display charts in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data\n",
    "\n",
    "We will begin by creating some linear data with gaussian noise added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "#We will create some one dimensional data with a bit of noise\n",
    "num_points = 50\n",
    "# Generating evenly spread Xs\n",
    "X = np.linspace(0,100,num_points).reshape(num_points,1)\n",
    "# generating rondom target data for Xs with the second part being a nois\n",
    "y = (4 + 3 * X) + 25*np.random.randn(num_points, 1)\n",
    "\n",
    "#Plot the data, we want to divine a best fit\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.title('Training Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "The function below accepts training data X and targets y and returns best fit parameters theta. (In the case of a simple one dimensional regression for example, we will return two theta parameters, the bias and linear gradient).\n",
    "\n",
    "Notice above that we have 50 training points, so X will be a numpy array of shape (50,1)\n",
    "\n",
    "### To do: Write a function to fit the data and return optimal parameters\n",
    "\n",
    "Return the best fit parameters theta, for a single linear regression this will be a numpy array of shape (2,1).\n",
    "\n",
    "Steps:\n",
    "\n",
    "a) Prepend a vector of ones onto the training data (to include the bias).\n",
    "\n",
    "b) Calculate theta_best using the normal equation.\n",
    "\n",
    "Consider making the function as general as possible by accepting higher dimensional inputs to save further work when you do multi-linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_linear_regression(X, y):\n",
    "    #[your code here\n",
    "    ones=np.ones(X.shape)\n",
    "    X=np.append(ones,X,axis=1)\n",
    "    print(X)\n",
    "    coeff=np.linalg.inv((X.T).dot(X)).dot(X.T).dot(y).T\n",
    "    return coeff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function to return predictions\n",
    "\n",
    "### To do: \n",
    "\n",
    "Given parameters theta and data X_test, return predictions y_pred\n",
    "(don't forget about the bias).\n",
    "\n",
    "X_test will be of dimension (50,1), and in the case of single linear regression the return value theta of dimension (2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(theta, X_test):\n",
    "    ones=np.ones(X_test.shape)\n",
    "    X_test=np.append(ones,X,axis=1)\n",
    "    y_test=theta.dot(X_test.T)\n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the predictions on X\n",
    "\n",
    "### To do: Use your two functions to create predictions given the data X\n",
    "\n",
    "Call the variable containing the predictions ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.           0.        ]\n",
      " [  1.           2.04081633]\n",
      " [  1.           4.08163265]\n",
      " [  1.           6.12244898]\n",
      " [  1.           8.16326531]\n",
      " [  1.          10.20408163]\n",
      " [  1.          12.24489796]\n",
      " [  1.          14.28571429]\n",
      " [  1.          16.32653061]\n",
      " [  1.          18.36734694]\n",
      " [  1.          20.40816327]\n",
      " [  1.          22.44897959]\n",
      " [  1.          24.48979592]\n",
      " [  1.          26.53061224]\n",
      " [  1.          28.57142857]\n",
      " [  1.          30.6122449 ]\n",
      " [  1.          32.65306122]\n",
      " [  1.          34.69387755]\n",
      " [  1.          36.73469388]\n",
      " [  1.          38.7755102 ]\n",
      " [  1.          40.81632653]\n",
      " [  1.          42.85714286]\n",
      " [  1.          44.89795918]\n",
      " [  1.          46.93877551]\n",
      " [  1.          48.97959184]\n",
      " [  1.          51.02040816]\n",
      " [  1.          53.06122449]\n",
      " [  1.          55.10204082]\n",
      " [  1.          57.14285714]\n",
      " [  1.          59.18367347]\n",
      " [  1.          61.2244898 ]\n",
      " [  1.          63.26530612]\n",
      " [  1.          65.30612245]\n",
      " [  1.          67.34693878]\n",
      " [  1.          69.3877551 ]\n",
      " [  1.          71.42857143]\n",
      " [  1.          73.46938776]\n",
      " [  1.          75.51020408]\n",
      " [  1.          77.55102041]\n",
      " [  1.          79.59183673]\n",
      " [  1.          81.63265306]\n",
      " [  1.          83.67346939]\n",
      " [  1.          85.71428571]\n",
      " [  1.          87.75510204]\n",
      " [  1.          89.79591837]\n",
      " [  1.          91.83673469]\n",
      " [  1.          93.87755102]\n",
      " [  1.          95.91836735]\n",
      " [  1.          97.95918367]\n",
      " [  1.         100.        ]]\n",
      "[[-7.52099335  3.19297851]]\n",
      "[[ -7.52099335]\n",
      " [ -1.00471068]\n",
      " [  5.51157199]\n",
      " [ 12.02785466]\n",
      " [ 18.54413733]\n",
      " [ 25.06042   ]\n",
      " [ 31.57670266]\n",
      " [ 38.09298533]\n",
      " [ 44.609268  ]\n",
      " [ 51.12555067]\n",
      " [ 57.64183334]\n",
      " [ 64.15811601]\n",
      " [ 70.67439868]\n",
      " [ 77.19068135]\n",
      " [ 83.70696401]\n",
      " [ 90.22324668]\n",
      " [ 96.73952935]\n",
      " [103.25581202]\n",
      " [109.77209469]\n",
      " [116.28837736]\n",
      " [122.80466003]\n",
      " [129.3209427 ]\n",
      " [135.83722536]\n",
      " [142.35350803]\n",
      " [148.8697907 ]\n",
      " [155.38607337]\n",
      " [161.90235604]\n",
      " [168.41863871]\n",
      " [174.93492138]\n",
      " [181.45120404]\n",
      " [187.96748671]\n",
      " [194.48376938]\n",
      " [201.00005205]\n",
      " [207.51633472]\n",
      " [214.03261739]\n",
      " [220.54890006]\n",
      " [227.06518273]\n",
      " [233.58146539]\n",
      " [240.09774806]\n",
      " [246.61403073]\n",
      " [253.1303134 ]\n",
      " [259.64659607]\n",
      " [266.16287874]\n",
      " [272.67916141]\n",
      " [279.19544407]\n",
      " [285.71172674]\n",
      " [292.22800941]\n",
      " [298.74429208]\n",
      " [305.26057475]\n",
      " [311.77685742]]\n"
     ]
    }
   ],
   "source": [
    "#[your code here]\n",
    "my_coeffs=simple_linear_regression(X,y)\n",
    "print(my_coeffs)\n",
    "ypred=pred(my_coeffs,X).T\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now use scikit learn\n",
    "\n",
    "### To do:\n",
    "\n",
    "1) Create a linear regression model\n",
    "\n",
    "2) fit the model using X and y\n",
    "\n",
    "3) Predict y given the fitted model and X - put this in variable y_lr\n",
    "\n",
    "4) Calculate Mean squared errors for the hand coded and the scikit models, call the variables mse and mse_lr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "From scikit lr model:\n",
      " None\n",
      "Score =  0.9211291030520586\n",
      "Coef =  [[0.         3.19297851]]\n"
     ]
    }
   ],
   "source": [
    "#[your code here]\n",
    "def typical_linear_regression(X,y):\n",
    "    ones=np.ones(X.shape)\n",
    "    X=np.append(ones,X,axis=1)\n",
    "    reg = LinearRegression().fit(X,y)\n",
    "    print(\"Score = \",reg.score(X,y))\n",
    "    print(\"Coef = \", reg.coef_)\n",
    "    return reg.predict(X)\n",
    "#Solution\n",
    "def calculate_mse(y_true,y_pred):\n",
    "    mse = np.subtract(y_true.T,y_pred)\n",
    "    mse = np.square(mse)\n",
    "    mse = np.sum(mse)\n",
    "    print(mse.shape)\n",
    "#     mse = mse/\n",
    "#     return(mse)\n",
    "# print(\"From my model:\",calculate_mse(y,y_lr))\n",
    "print(\"From scikit lr model:\\n\",calculate_mse(y,ypred))\n",
    "y_lr=typical_linear_regression(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine results:\n",
    "\n",
    "Nothing to do here, if everything has been carried out correctly you should see the training data (as blue dots). The line of best fit from your own code ypred, and the best fit from your scikit learn implementation (green x's).\n",
    "\n",
    "The red line should be a good fit to the blue training data and the green x's should be an exact match to the red line. The mean squared errors are also reported and should be essentially the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "#We wish to plot the variables you created ypred, y_lr\n",
    "#Plot the predictions and MSE's - you should have a staight line fit and the line exactly\n",
    "#matching the crosses through the data, the MSE's should also be equal to a large number of decimal places\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.plot(X, ypred, 'r-')\n",
    "plt.plot(X, y_lr, 'g+')\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.show()\n",
    "# print(\"MSE - Sci kit\",mse_lr,\",\",\"MSE - Hand-Coded\",mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression\n",
    "\n",
    "We will now examine multiple linear regression, this time we will go through the Scikit workflow. To avoid data cleaning/ munging etc. we will use the diabetes data from Scikit-learn. The dataset contains ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements from diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_diabetes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5d5d7600c257>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##PLEASE DO NOT CHANGE THIS CELL: Load the diabetes data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdiabetes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_diabetes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_diabetes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiabetes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_diabetes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiabetes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_diabetes' is not defined"
     ]
    }
   ],
   "source": [
    "##PLEASE DO NOT CHANGE THIS CELL: Load the diabetes data\n",
    "diabetes = load_diabetes()\n",
    "X_diabetes = diabetes['data']\n",
    "y_diabetes = diabetes['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End to End Diabetes\n",
    "\n",
    "### To Do:\n",
    "\n",
    "1) Create a train/test split - hint use train_test_split(), choose a test_sise=0.2, and a random_state=42, call the variable containing the test targets y_test.\n",
    "\n",
    "2) Linear regression parameter outputs are dependent on the scale of the features, use the StandardScalar to fit/ transform the training data and to fit the test data. (Why do we not also fit on the test data?)\n",
    "\n",
    "3) Create a linear regression model, fit it, and store predictions for X_test in a variable called y_lr_diabetes.\n",
    "\n",
    "4) How good is your trained model on the test data? Calculate the mean squared error between y_test and y_lr_diabetes, store the result in a variable called mse_lr_diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[your code here]: Split\n",
    "\n",
    "#[your code here]: Standardise\n",
    "\n",
    "#[your code here]: Fit\n",
    "\n",
    "#[your code here]: MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now use your own model as before.\n",
    "\n",
    "If you already coded your function to handle multiple dimensions you have no alterations to make to the simple linear regression function, otherwise you will need to generalise a little for higher dimensional data.\n",
    "\n",
    "### To Do:\n",
    "\n",
    "1) Use the simple linear regression function with training inputs from your standard scalar to fit parameters theta\n",
    "\n",
    "2) Make your predictions given your parameters and input test data\n",
    "\n",
    "3) Calculate the mean squared errors of your predictions\n",
    "\n",
    "Store your predictions for X_test in a variable called y_pred_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[your code here]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine results - Your model v Scikit\n",
    "\n",
    "Because we are dealing with multiple dimensions visualisation is less simple, below we show our model compared the the Scikit learn version, we will do a scatter graph against the y_test results. We would hope the pattern is close to identical. \n",
    "\n",
    "We also show the mean squared errors on the test set of the two models which should be the same to a high precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PLEASE DO NOT CHANGE THIS CELL\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, sharex=True)\n",
    "axes[0].scatter(ypred_diabetes,y_test)\n",
    "axes[0].set_title(\"Linear Regression\")\n",
    "axes[1].scatter(y_lr_diabetes,y_test)\n",
    "axes[1].set_title(\"SKLearn Linear Regression\")\n",
    "\n",
    "\n",
    "print(\"MSE-hand coded\",mse_ypred_diabetes, \"MSE-Sci-Kit\",mse_lr_diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear Regression by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now examine non-linear regression, here using basic polynomial features, the underlying function is a cubic, again noise is added. In terms of training our model only sees the noisy data. The 'ground truth', test data would be the underlying cubic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "\n",
    "num_points = 50\n",
    "X = np.linspace(-3,5,num_points).reshape(num_points,1) \n",
    "y_clean = X**3 \n",
    "y = y_clean + 25*np.random.randn(num_points, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do:\n",
    "\n",
    "Scale the training data X using the standard scalar. Store in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[your code here]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nothing to do, we will plot the scaled training data.\n",
    "\n",
    "The x axis should have been rescaled with axis extremes around -1.5/1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "\n",
    "#Plot the data\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For interest only \n",
    "\n",
    "In the cell below we show the training data X, the training targets y and the features we would use for polynomial regression. It is simply a linear regression with input features transformed, we create these to hand-code the Scikit implementation above. In the cell below we show the input features used for the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "data = pd.DataFrame(np.column_stack([X,y]),columns=['X','y'])\n",
    "\n",
    "\n",
    "for i in range(2,30):  \n",
    "    colname = 'X_%d'%i      \n",
    "    data[colname] = data['X']**i\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For interest only \n",
    "\n",
    "Here we have a function that performs linear regression and plots and calculates the residual sum of squares on a number of polynomial models.\n",
    "\n",
    "### Question:\n",
    "\n",
    "What is the residual sum of squares? How is it different to mean squared error and why do we care about all these error functions? If we performed classification would we have a different error function for example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "def linear_regression(data, power, models_to_plot):\n",
    "    predictors=['X']\n",
    "    if power>=2:\n",
    "        predictors.extend(['X_%d'%i for i in range(2,power+1)])\n",
    "    \n",
    "    #Fit the model\n",
    "    linreg = LinearRegression() #normalize=True)\n",
    "    linreg.fit(data[predictors],data['y'])\n",
    "    y_pred = linreg.predict(data[predictors])\n",
    "    \n",
    "    #Check if a plot is to be made for the entered power\n",
    "    if power in models_to_plot:\n",
    "        plt.subplot(models_to_plot[power])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['X'],y_pred)\n",
    "        plt.plot(data['X'],data['y'],'.')\n",
    "        #plt.plot(data['X'],y_clean)\n",
    "        plt.title('Plot for power: %d'%power)\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    ret.extend([linreg.intercept_])\n",
    "    ret.extend(linreg.coef_)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "In the cell below we show the fit for a number of polynomials of varying degrees. \n",
    "\n",
    "### Question:\n",
    "\n",
    "Which do you think show bias? And which over-fitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "#Initialize a dataframe to store the results:\n",
    "col = ['rss','intercept'] + ['coef_X_%d'%i for i in range(1,30)]\n",
    "ind = ['model_pow_%d'%i for i in range(1,30)]\n",
    "coef_matrix_simple = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "#Define the powers for which a plot is required:\n",
    "models_to_plot = {1:231,3:232,6:233,12:234,18:235,29:236}\n",
    "\n",
    "#Iterate through all powers and assimilate results\n",
    "for i in range(1,30):\n",
    "    coef_matrix_simple.iloc[i-1,0:i+2] = linear_regression(data, power=i, models_to_plot=models_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficient Matrix\n",
    "\n",
    "The cell below shows the residual sum of squares and coefficients for each of the regressions. For example the cubic has an intercepts and three coeffients. \n",
    "\n",
    "### Questions:\n",
    "\n",
    "1) What do you notice about the residual sum of squares as the degree of the polynomial increases?\n",
    "\n",
    "2) What do you notice about the size of the coefficents as the degree of the polynomial increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "coef_matrix_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In reality managing calculating parameters by using the normal equation becomes unwieldy as data scales:\n",
    "\n",
    "\\begin{align}\n",
    "\\theta = (X'X)^{-1}Xy\n",
    "\\end{align}\n",
    "\n",
    "1) Why is this?\n",
    "\n",
    "Alternative methods are to use gradient descent. In the example below we wish to fit the data using batch gradient descent. \n",
    "\n",
    "### To Do:\n",
    "\n",
    "Be sure of the differences between batch, mini-batch and stochastic gradient descent.\n",
    "\n",
    "\n",
    "To keep things straightforward we shall go back to our original linear data plus noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "num_points = 50\n",
    "X = np.linspace(0,100,num_points).reshape(num_points,1)\n",
    "y = (4 + 3 * X) + 25*np.random.randn(num_points, 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "#Plot the data, we want to divine a best fit\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.title('Training Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement batch gradient descent\n",
    "\n",
    "### To Do:\n",
    "\n",
    "1) Derive the gradient of the error with respect to parameters in linear regression.\n",
    "\n",
    "2) Implement gradient linear regression in code. \n",
    "\n",
    "### Hints...\n",
    "\n",
    "2a) First add a bias as you did previously\n",
    "\n",
    "2b) Create a loop ranging to the number of iterations we wish to perform (n_iterations)\n",
    "\n",
    "2c) Calculate the gradients in the loop\n",
    "\n",
    "2d) Update variable theta_best by taking a step-size eta in the opposite direction to the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_linear_regression(X, y, n_iterations):\n",
    "    eta = 0.01\n",
    "    n_iterations = n_iterations \n",
    "    m = X.shape[0]\n",
    "    theta_best = np.random.randn(X.shape[1]+1,1)\n",
    "    \n",
    "    #[your code here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "theta_best_10 = gradient_linear_regression(X, y,10)\n",
    "theta_best_30 = gradient_linear_regression(X, y,30)\n",
    "theta_best_50 = gradient_linear_regression(X, y,30)\n",
    "theta_best_100 = gradient_linear_regression(X, y,100)\n",
    "theta_best_500 = gradient_linear_regression(X, y,500)\n",
    "\n",
    "ypred_10 = pred(theta_best_10,X)\n",
    "ypred_30 = pred(theta_best_30,X)\n",
    "ypred_50 = pred(theta_best_50,X)\n",
    "ypred_100 = pred(theta_best_100,X)\n",
    "ypred_500 = pred(theta_best_500,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Batch gradient descent\n",
    "\n",
    "The training data is fitted by linear regression with gradient descent, however we have used a different number of epochs. That is the number of iterations before we stop learning.\n",
    "\n",
    "### Questions:\n",
    "\n",
    "1) What do you notice as the epochs increase?\n",
    "\n",
    "2) How would you decide the number of epochs to perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "plt.plot(X, y, 'b.', label='Training Data')\n",
    "plt.plot(X, ypred_10, label='Prediction 10 epochs')\n",
    "plt.plot(X, ypred_30, label='Prediction 30 epochs')\n",
    "plt.plot(X, ypred_50, label='Prediction 50 epochs')\n",
    "plt.plot(X, ypred_100,label='Prediction 100 epochs')\n",
    "plt.plot(X, ypred_500, label='Prediction 500 epochs')\n",
    "\n",
    "\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "\n",
    "plt.title(\"Linear Regression using Gradient Descent\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
